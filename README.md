# FReLU-torch
A implementation of Flexible ReLU activation function with PyTorch
